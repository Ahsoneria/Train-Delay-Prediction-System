# -*- coding: utf-8 -*-
"""ScheduleData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fDmHn40B2KNBiWYf0A2hbf3LdBh7jh6a
"""

import pandas as pd
import numpy as np
import json
import requests as r
from bs4 import BeautifulSoup

from lxml import html

def parseStationCell(cell):
  station_name = cell.find('abbr').text
  station_code = cell.find('small').text
  return [station_name, station_code]
def parseScheduleCell(cell):
  human_delay = cell.find('small')
  human_delay_text = human_delay.text
  human_delay.decompose()
  data = cell.text.split('/')
  scheduled = 'NA'
  actual = 'NA'
  if len(data) > 0:
    scheduled=data[0]
  if len(data) > 1:
    actual=data[1]
  return [scheduled, actual, human_delay_text]

def parseDelayHTML(delay_html):
  root = html.fromstring(delay_html)
  element = root.xpath("//tbody")[0]
  soup = BeautifulSoup(html.tostring(element), 'html.parser')
  parsedObj = []
  rows = soup.find_all('tr')
  for row in rows:
    cells = row.find_all('td')
    if len(cells) == 4:
      data = []
      data.extend(parseStationCell(cells[0]))
      data.extend(parseScheduleCell(cells[1]))
      data.extend(parseScheduleCell(cells[2]))
      data.extend(cells[3].text)
      parsedObj.append(data)
  return parsedObj

# parseDelayHTML(delay_html)
columns = ['Station Name', 'Station Code', 'Scheduled Arrival', 'Actual Arrival', 'Arrival Delay Human Format', 'Scheduled Departure', 'Actual Departure', 'Departure Delay Human Format', 'Platform No']
# pd.DataFrame(parseDelayHTML(delay_html), columns=columns)

import time
import concurrent.futures

MAX_THREADS = 30
# MAX_THREADS = 10

def scrapeTrain(train_no):
  # start_dates_url = 'https://runningstatus.in/history/{}/1262284200-1604514600'
  # start_dates_url = 'https://runningstatus.in/history/{}/1514745000-1604514600'
  # start_dates_url = 'https://runningstatus.in/history/{}/1546281000-1604514600'
  start_dates_url = 'https://runningstatus.in/history/{}/1514745000-1546281000'
  html = r.get(start_dates_url.format(train_no)).content
  soup = BeautifulSoup(html, 'html.parser')
  start_date_table = soup.find_all('table')[1]
  start_date_url_selector = 'tbody > tr > td:nth-child(1) > a'
  invalid_train_selector = 'div.card-body > table > tbody > tr > td > div'

  elem = soup.select_one(invalid_train_selector)

  if elem and elem.text.strip() == 'Time Table not available for this train':
    return {}

  base_url = 'http://www.runningstatus.in'
  headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}
  
  train_info = {}
  train_run_page_list = []

  for elem in start_date_table.select(start_date_url_selector):
    train_run_page_list.append(base_url+elem.get('href'))
  
  threads = min(MAX_THREADS, len(train_run_page_list))

  def scrapeDay(elem):
    date = elem.split('-')[-1]
    delay_html = r.get(elem, headers=headers).content
    train_info[date] = parseDelayHTML(delay_html)
  with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
    executor.map(scrapeDay, train_run_page_list)
  return train_info

with open('./train_list.json', 'r') as file:
  trains = json.loads(file.read())

"""Set my_trains = trains[x1:x2]
where x1 & x2 are the slices you want to download
"""

import os
downloaded_files = [x.split('_')[0] for x in os.listdir() if x.split('_',1)[-1] == '2018_2019.json']
# downloaded_files
my_trains = trains[:1700]
remaining_files = [x for x in my_trains if x not in downloaded_files]
remaining_files = np.random.permutation(remaining_files)
total_time = 0

import signal, time, random

class TimeoutError (RuntimeError):
    pass

def handler (signum, frame):
    raise TimeoutError()

signal.signal (signal.SIGALRM, handler)

while len(remaining_files) > 1:
  for (ind, train_no) in enumerate(remaining_files, 1):
    try:
      signal.alarm(200)
      t0 = time.time()
      train_data = scrapeTrain(train_no)
      with open('{}_2018_2019.json'.format(train_no), 'w+') as file:
        file.write(json.dumps(train_data))
      t1 = time.time()
      total_time += t1-t0
      print('Done {} of {} in {} secs, Total: {}, Avg: {}'.format(ind, len(remaining_files), round(t1-t0, 2), round(total_time,2), round(total_time/ind,2)))
    except TimeoutError as ex:
      print('timeout', ind, train_no)
  downloaded_files = [x.split('_')[0] for x in os.listdir() if x.split('_',1)[-1] == '2018_2019.json']
  remaining_files = [x for x in my_trains if x not in downloaded_files]
  remaining_files = np.random.permutation(remaining_files)
  total_time = 0